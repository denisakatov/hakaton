{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hackaton.ipynb",
      "provenance": [],
      "mount_file_id": "1mIKONZ85PD7vzsl9VcPPU-58tns_Fpsb",
      "authorship_tag": "ABX9TyPkKGLR191sm9CBI8w25sgL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denisakatov/hakaton/blob/main/hackaton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql1FSySGATXR"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('tokenizer_tf2_qa')\n",
        "model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
        "\n",
        "questions = ['How long did it take to find the answer?',\n",
        "             'What\\'s the answer to the great question?',\n",
        "             'What\\'s the name of the computer?']\n",
        "\n",
        "paragraph = '''<p>The computer is named Deep Thought.</p>.\n",
        "<p>After 46 million years of training it found the answer.</p>\n",
        "<p>However, nobody was amazed. The answer was 42.</p>'''\n",
        "\n",
        "\n",
        "for question in questions:\n",
        "  print(question)\n",
        "  question_tokens = tokenizer.tokenize(question)\n",
        "  paragraph_tokens = tokenizer.tokenize(paragraph)\n",
        "  tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + paragraph_tokens + ['[SEP]']\n",
        "  input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  input_mask = [1] * len(input_word_ids)\n",
        "  input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(paragraph_tokens) + 1)\n",
        "\n",
        "  input_word_ids, input_mask, input_type_ids = map(lambda t: tf.expand_dims(tf.convert_to_tensor(t, dtype=tf.int32), 0), (input_word_ids, input_mask, input_type_ids))\n",
        "  outputs = model([input_word_ids, input_mask, input_type_ids])\n",
        "#  using `[1:]` will enforce an answer. `outputs[0][0][0]` is the ignored '[CLS]' token logit\n",
        "  short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
        "  short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
        "  answer_tokens = tokens[short_start: short_end + 1]\n",
        "  answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "  print(f'Question: {question}')\n",
        "  print(f'Answer: {answer}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFw5j6ElA2nf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}